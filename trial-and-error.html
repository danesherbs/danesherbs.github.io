<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dane Sherburn - Work</title>
  <link href="style.css" rel="stylesheet" />
</head>

<body>

  <table>
    <tr>
      <td>
        <header>
          Trial and error
        </header>
        </br>
        September 2020
        </br>
        </br>
        How likely is it that a solution to a problem is optimal? Unless it's a well-behaved mathematical problem, it's
        stupefyingly improbable [1]. We're therefore
        in the business of continually trying out new ideas and error correcting when solving a problem. Trial and error
        is surprisngly central to many good ideas, and a lot of good advice boils down to one thing:
        tightening the trial and error loop.

        <img src="trial-and-error.jpeg" style="width: 12em;" class="center" alt="trial-and-error">

        When writing software, it's best to build the absolute simplest end-to-end system as fast as possible. This
        leads you to ask the right questions earlier, like "do we really want to solve X, or do we actually want to
        solve Y?" and "how do we even measure our progress on problem Y?". The tighter the trial and error loop, the
        better.

        <blockquote>
          “The question is, how fast can you discover that the thing you’re working on is the wrong thing to be working
          on.”
          <br>
          -- Astro Teller, Captain of Moonshots at Google X
        </blockquote>

        You may even know this to be true from experience if you've written software. How much easier is it to write
        software with a REPL than without?

        </br>
        </br>

        There's a number of suprising places this principle arises. Paul Graham advises start ups to release early
        to quickly discover any major flaws, like a poor business idea or problems among founders. Neural networks first
        start with random parameters, but the error in their predictions is measured and parameters are iteratively
        updated via backpropagation. Likewise, solutions to intractable PDEs are approximated with iterative schemes
        like Gauss-Seidel and Lax Wendroff.

        </br>
        </br>

        It's also common in numerical analysis. The idea is to try a solution, measure the error, and try a better
        solution in the next iteration. Iterative schemes for solving PDEs, like Guass-Seidel and Lax Wendroff, follow
        this pattern. As does training neural networks: initially the parameters are random, but the error is measured
        and parameters are updated using back-propagation.

        <!-- Numerical analysis - absence of an analytic solution - try a solution, measure error, try a better solution next
        time. Iterative schemes for solving PDEs e.g. Gauss-Seidel, Lax Wendroff. Training neural networks - guess
        parameters, error correct using back-propagation. Expectation Maximisation algorithm e.g. Gaussian mixture
        models. -->

        </br>
        </br>

        Paul Graham's advice to release early is the same sentiment. "Release early" is common start up advice from Paul
        Graham and Reid Hoffman. Paul Graham PG's "talk to
        users" is telling you which signal to pay attention to. PG's "release early" is
        shortening the
        feedback loop. PG's "determination is more important than intelligence" is saying it's more important to execute
        the loop many times than to reduce the work done per step.

        </br>
        </br>

        Trial and error is also central to many good ideas, namely science and democracy. In science, candidate theories
        a chosen, experiments are performed and theories are error-corrected. In
        democracy, the best candidate is voted into power, their policy executed, and we error-correct by not voting in
        cadidates that repeat previous mistakes. This only works when not only the candidate is replaced, but their
        policies also.

        </br>
        </br>

        Determination, keep executing the loop, more cycles almost always beats more work done per cycle
        </br>
        <ul>
          <li>Speed is time/step.</li>
          <li>Intelligence is work done/step i.e. how good error correction is.</li>
          <li>Determination is keep executing the loop. It's more important to keep executing the loop than to increase
            work done per step.</li>
          <li>Not getting discouraged while iterating the loop</li>
          <li>Whenever you can find a feedback loop you can be successful</li>
        </ul>

        </br>
        </br>

        <ul>
          <li>Science - hypothesis, experiment, correct theory</li>
          <li>Democracy - David Deutsch - pick best candidate, error correct. Peaceful transition of power is key to the
            loop continuing to execute.</li>
          <li>Start ups - Paul Graham - talk to users (signal you should pay attention to), be determined (keep
            executing)</li>
          <li>Engineering - Google X - fail fast - “The question is,” said Dr. Astro Teller, “how fast can you
            discover that the thing you’re working on is the wrong thing to be working on.”</li>
          <li>Growth Mindset - Carol Dweck - everyones first iterations of the loop look bad - experts are those that
            just kept executing the loop</li>
          <li>Elon Musk - Tesla - optimise for tighter feedback-loop between assembly line and engineers</li>
          <li>Job applications - applying and failing, then gaining relevant experience, as opposed to preparing for
            ages in isolation and realising that you should have been doing something completely different</li>
          <li>Programming - easier to program with a REPL</li>
          <li>Project management - build the simplest end-to-end system first. This leads to you asking the right
            questions earlier, like "do we really want to solve X, or do we actually want to solve Y?" and "how do we
            even measure our progress on problem Y?"</li>
          <li>Numerical analysis - absence of an analytic solution - try a solution, measure error, try a better
            solution next time. Iterative schemes for solving PDEs e.g. Gauss-Seidel, Lax Wendroff. Training neural
            networks - guess parameters, error correct using back-propagation. Expectation Maximisation algorithm e.g.
            Gaussian mixture models.</li>
        </ul>

        </br>
        </br>

        A lot of good advice boils down to having a tighter trial and error loop (e.g. Google X "fail fast",
        PG's "release early"), or to keep executing it (e.g. Carol Dweck's Growth Mindset, PG's "determination matters
        more than intelligence").

        </br>
        </br>

        Have a tight trial and error loop, and don't give up.

        </br>
        </br>

        <b>Footnotes</b>
        </br>
        [1] Sometimes mathematics allows for analytic solutions. This is
        quite rare, though. For example, there's an analytic solution for the parameters in linear regression (the
        normal equations), but as soon as you deviate slightly and add a non-linear activation function (e.g. the
        logistic function), there's no longer an anlytic solution. Instead, numerical methods are used (e.g. gradient
        descent), which uses trial and error to find successively better parameters.



      </td>
    </tr>
  </table>

</body>

</html>