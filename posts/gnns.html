<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dane Sherburn - Work</title>
  <link href="../style.css" rel="stylesheet" />
</head>

<body>
  <a href="../projects.html">Projects</a> / <a href="../blog.html">Blog</a> / <a href="../links.html">Links</a>
  </br>
  </br>

  <header>
    Evaluating graph neural networks
  </header>
  </br>
  February 2020
  </br>
  </br>
  It turns out that many popular datasets used to evaluate graph neural networks are too small. Papers often
  report results on a single data split (usually <a href="https://arxiv.org/abs/1603.08861">Yang et al., 2016</a>)
  and claim their model is superior with statistical significance. But their conclusions could have been entirely
  different on another data split!
  </br>
  </br>
  For example, take different splits of a popular dataset, Cora, and two popular networks, the graph convolutional
  network (GCN) and graph attention network (GAT).
  </br>
  </br>
  <table>
    <tr>
      <th>Data split</th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>D</th>
    </tr>
    <tr>
      <td>GCN</td>
      <td><b>82.22% ± 0.31%</b></td>
      <td>78.79% ± 2.63%</td>
      <td><b>80.50% ± 0.86%</b></td>
      <td>77.55% ± 3.53%</td>
    </tr>
    <tr>
      <td>GAT</td>
      <td>79.79% ± 1.43%</td>
      <td><b>84.06% ± 0.74%</b></td>
      <td>78.42% ± 2.45%</td>
      <td><b>83.81% ± 0.69%</b></td>
    </tr>
    <tr>
      <td>p-value</td>
      <td>1.333e-07</td>
      <td>7.152e-09</td>
      <td>1.171e-03</td>
      <td>7.153e-08</td>
    </tr>
  </table>
  </br>
  If we used data split A, we'd say GCN is better than GAT. Whereas if we happened to use data split B, we'd
  conclude the opposite. In both cases we have incredibly low p-values.
  </br>
  </br>
  This isn't true for just Cora, but many popular datasets e.g. Citeseer and Pubmed. It seems like this is a
  mistake that's wide-spread, even making it's way into peer-reviewed conferences [1][2]. A simple way to correct
  this mistake is to use k-fold cross-validation.
  </br>
  </br>
  <b>Code</b>
  </br>
  <a href="https://github.com/danesherbs/citation-networks">https://github.com/danesherbs/citation-networks</a>.
  You can reproduce all experiments in this post by running <font face="Courier New">reproduce.sh</font>.
  </br>
  </br>
  <b>References</b>
  </br>
  [1] Semi-Supervised Classification with Graph Convolutional Networks, ICLR 2017, <a
    href="https://arxiv.org/abs/1609.02907">https://arxiv.org/abs/1609.02907</a>
  </br>
  [2] Deep Graph Infomax, ICLR 2019, <a href="https://arxiv.org/abs/1809.10341">https://arxiv.org/abs/1809.10341</a>
</body>

</html>